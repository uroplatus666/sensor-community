import logging
import sys
import os
import json
import datetime
from datetime import timedelta, timezone

from scraper import scrape_data
from processor import run_processing
from uploader import run_upload

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)


def load_config():
    with open('app/config.json', 'r') as f:
        config = json.load(f)
    env_token = os.getenv('MAPBOX_TOKEN')
    if env_token:
        config['mapbox_token'] = env_token
    return config


# --- –†–ê–ë–û–¢–ê –°–û STATE-–§–ê–ô–õ–û–ú ---

def get_state_file_path(config):
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å–æ—Å—Ç–æ—è–Ω–∏—è –≤–Ω—É—Ç—Ä–∏ –ø–∞–ø–∫–∏ data"""
    data_dir = config.get('data_dir', 'data')
    os.makedirs(data_dir, exist_ok=True)
    return os.path.join(data_dir, 'state.json')


def load_state(state_path):
    """–ß–∏—Ç–∞–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∏–∑ JSON —Ñ–∞–π–ª–∞."""
    if not os.path.exists(state_path):
        return {}
    try:
        with open(state_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        logging.warning(f"Failed to load state file {state_path}: {e}. Starting clean.")
        return {}


def save_state(state_path, state_data):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ –≤ JSON —Ñ–∞–π–ª."""
    try:
        with open(state_path, 'w', encoding='utf-8') as f:
            json.dump(state_data, f, indent=4, ensure_ascii=False)
        logging.info(f"üíæ State saved to {state_path}")
    except Exception as e:
        logging.error(f"Failed to save state file: {e}")


# --- –õ–û–ì–ò–ö–ê –†–ê–°–ß–ï–¢–ê –î–ê–¢ ---

def prepare_schedule_and_state(config, current_state):
    """
    –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç target_start –Ω–∞ –æ—Å–Ω–æ–≤–µ state.json.
    """
    today = datetime.datetime.now(timezone.utc).date()
    today_str = today.strftime("%Y-%m-%d")

    logging.info(f"üìÖ Daily Job: Today is {today_str}")

    sensor_types = ['sds', 'bme']

    # –ì–æ—Ç–æ–≤–∏–º –æ–±—ä–µ–∫—Ç –±—É–¥—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è
    new_state = current_state.copy()
    at_least_one_task = False

    for s_type in sensor_types:
        if s_type not in config.get('sensors', {}):
            continue

        if s_type not in new_state:
            new_state[s_type] = {}

        sensors = config['sensors'][s_type]

        for sensor_id, dates in sensors.items():
            sensor_id_str = str(sensor_id)

            # --- 1. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–æ—á–∫–∏ —Å—Ç–∞—Ä—Ç–∞ ---

            # –î–∞—Ç–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
            try:
                config_start_dt = datetime.datetime.strptime(dates['start'], "%Y-%m-%d").date()
            except ValueError:
                config_start_dt = today

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º State
            sensor_state = new_state[s_type].get(sensor_id_str, {})
            last_downloaded_str = sensor_state.get('last_downloaded')

            target_start = config_start_dt

            if last_downloaded_str:
                # –í–ê–ñ–ù–û: –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –±—ã–ª–∏ —Å–∫–∞—á–∞–Ω—ã, –Ω–∞—á–∏–Ω–∞–µ–º —Å–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ –¥–Ω—è
                try:
                    last_dt = datetime.datetime.strptime(last_downloaded_str, "%Y-%m-%d").date()
                    target_start = last_dt + timedelta(days=1)
                    logging.info(
                        f"Sensor {sensor_id}: Resuming from state. Last downloaded: {last_dt}. Next start: {target_start}")
                except ValueError:
                    logging.warning(f"Sensor {sensor_id}: Corrupted date in state. Using config start.")
            else:
                logging.info(f"Sensor {sensor_id}: No history in state. Starting from config: {config_start_dt}")
                sensor_state['initial_start'] = config_start_dt.strftime("%Y-%m-%d")

            # –ó–∞—â–∏—Ç–∞ –æ—Ç –¥–∞—Ç –≤ –±—É–¥—É—â–µ–º
            if target_start > today:
                logging.info(
                    f"Sensor {sensor_id}: Data is up to date (Target {target_start} > Today). Skipping scrape.")
                # –°—Ç–∞–≤–∏–º –¥–∞—Ç—ã —Ç–∞–∫, —á—Ç–æ–±—ã —Å–∫—Ä–∞–ø–µ—Ä –Ω–∏—á–µ–≥–æ –Ω–µ –¥–µ–ª–∞–ª (start > end)
                config['sensors'][s_type][sensor_id]['start'] = (today + timedelta(days=1)).strftime("%Y-%m-%d")
                config['sensors'][s_type][sensor_id]['end'] = today_str
            else:
                # –ù–æ—Ä–º–∞–ª—å–Ω—ã–π —Ä–µ–∂–∏–º
                config['sensors'][s_type][sensor_id]['start'] = target_start.strftime("%Y-%m-%d")
                config['sensors'][s_type][sensor_id]['end'] = today_str
                at_least_one_task = True
                logging.info(
                    f"   üëâ Plan for {sensor_id}: {config['sensors'][s_type][sensor_id]['start']} -> {today_str}")

            # --- 2. –û–±–Ω–æ–≤–ª—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ (–ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ) ---
            # –°—á–∏—Ç–∞–µ–º, —á—Ç–æ –µ—Å–ª–∏ —Å–∫—Ä–∞–ø–µ—Ä –æ—Ç—Ä–∞–±–æ—Ç–∞–µ—Ç, —Ç–æ –º—ã —Å–∫–∞—á–∞–µ–º –≤—Å–µ –≤–ø–ª–æ—Ç—å –¥–æ 'today_str'
            sensor_state['last_downloaded'] = today_str
            sensor_state['last_run_timestamp'] = datetime.datetime.now().isoformat()

            new_state[s_type][sensor_id_str] = sensor_state

    return config, new_state, at_least_one_task


def main():
    logging.info("üöÄ Job started.")
    try:
        # 1. –ó–∞–≥—Ä—É–∑–∫–∞
        config = load_config()
        state_path = get_state_file_path(config)
        current_state = load_state(state_path)

        # 2. –†–∞—Å—á–µ—Ç
        config, pending_state, has_tasks = prepare_schedule_and_state(config, current_state)

        # 3. ETL –ü–∞–π–ø–ª–∞–π–Ω

        # --- A. SCRAPING ---
        if has_tasks:
            scrape_data(config)
            # –í–ê–ñ–ù–û: –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–µ–π—Ç –°–†–ê–ó–£ –ø–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–≥–æ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è.
            # –î–∞–∂–µ –µ—Å–ª–∏ –ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ —É–ø–∞–¥–µ—Ç, –º—ã –∑–∞–ø–æ–º–Ω–∏–º, —á—Ç–æ —Ñ–∞–π–ª—ã —É–∂–µ —É –Ω–∞—Å.
            save_state(state_path, pending_state)
        else:
            logging.info("üí§ Skipping scrape (everything up to date).")

        # --- B. PROCESSING ---
        # –ü—Ä–æ—Ü–µ—Å—Å–æ—Ä —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –ª–æ–∫–∞–ª—å–Ω—ã–º–∏ —Ñ–∞–π–ª–∞–º–∏, –æ–Ω –Ω–∞–π–¥–µ—Ç –≤—Å—ë, —á—Ç–æ –µ—Å—Ç—å –≤ –ø–∞–ø–∫–µ
        run_processing(config)

        # --- C. UPLOADING ---
        # –ê–ø–ª–æ–∞–¥–µ—Ä —Å–∞–º –ø—Ä–æ–≤–µ—Ä–∏—Ç —Å–µ—Ä–≤–µ—Ä –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã
        run_upload(config)

        logging.info("‚úÖ Job finished successfully.")

    except Exception as e:
        logging.critical(f"üî• Job failed: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()