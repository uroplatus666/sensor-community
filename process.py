import os, time, math, random, requests
import pandas as pd
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Optional, Tuple, Dict, List

# –ö–ª—é—á–∏
MAPBOX_TOKEN = "YOUR_TOKEN" # ‚Üê –í–ê–® —Ç–æ–∫–µ–Ω

#______________________–í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–∫–∞—á–µ–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º_____________________

def scan_dir(root_path, label):
    print(f'\nüöÄ {label}  ({root_path})')
    if not os.path.isdir(root_path):
        print('   ‚ùå –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞')
        return

    # —Ç–æ–ª—å–∫–æ –ø–æ–¥-–ø–∞–ø–∫–∏ 1-–≥–æ —É—Ä–æ–≤–Ω—è
    for entry in sorted(os.listdir(root_path)):
        sub = os.path.join(root_path, entry)
        if not os.path.isdir(sub):
            continue

        files = [f for f in os.listdir(sub) if os.path.isfile(os.path.join(sub, f))]
        # –ü–æ–¥—Å—á–µ—Ç —Å—É–º–º–∞—Ä–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–æ–∫ –≤–æ –≤—Å–µ—Ö —Ñ–∞–π–ª–∞—Ö
        total_lines = 0
        for f in files:
            file_path = os.path.join(sub, f)
            try:
                with open(file_path, 'r', encoding='utf-8') as file:
                    total_lines += sum(1 for _ in file)
            except (IOError, UnicodeDecodeError):
                print(f'        ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª: {f}')

        print(f'   üìÅ {entry}/  ‚Üí  {len(files)} —Ñ–∞–π–ª(–æ–≤), —Å—É–º–º–∞—Ä–Ω–∞—è –¥–ª–∏–Ω–∞: {total_lines} —Å—Ç—Ä–æ–∫')
        if files:
            print(f'        ‚ú®–ø—Ä–∏–º–µ—Ä: {files[0]}')

folder_sds = r'data\SDS011'
folder_bme = r'data\BME280'

scan_dir(folder_sds, 'SDS011')
scan_dir(folder_bme, 'BME280')

# ______________________–°–æ–∑–¥–∞–Ω–∏–µ –æ–±—â–µ–≥–æ —Ñ–∞–π–ª–∞________________________

def _read_sensor_csv(path):
    last_err = None
    try:
        df = pd.read_csv(path, sep=';')
        sub = df[['timestamp', 'lat', 'lon']].copy()

        # –ø—Ä–∏–≤–æ–¥–∏–º —Ç–∏–ø—ã (—É—á–∏—Ç—ã–≤–∞–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ –¥–µ—Å—è—Ç–∏—á–Ω—ã–µ –∑–∞–ø—è—Ç—ã–µ)
        sub['lat'] = pd.to_numeric(sub['lat'].astype(str).str.replace(',', '.'), errors='coerce')
        sub['lon'] = pd.to_numeric(sub['lon'].astype(str).str.replace(',', '.'), errors='coerce')
        sub['timestamp'] = pd.to_datetime(sub['timestamp'], errors='coerce', utc=False)

        sub = sub.dropna(subset=['timestamp', 'lat', 'lon'])

        return sub  # —É—Å–ø–µ—Ö
    except Exception as e:
        last_err = f'{type(e).__name__}: {e}'

    # –µ—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –ø–æ–¥–æ—à–ª–æ ‚Äî –¥–∞—ë–º –ø–æ–Ω—è—Ç—å –≤—ã–∑—ã–≤–∞—é—â–µ–º—É –∫–æ–¥—É
    raise RuntimeError(f'–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å {os.path.basename(path)}. –ü–æ—Å–ª–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞: {last_err}')

def process_root(root_path, sensor_type):
    """–°–æ–±–∏—Ä–∞–µ—Ç —Å—Ç—Ä–æ–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –¥–ª—è –æ–¥–Ω–æ–π –∫–æ—Ä–Ω–µ–≤–æ–π –ø–∞–ø–∫–∏ —Ç–∏–ø–∞ –¥–∞—Ç—á–∏–∫–∞."""
    rows = []
    if not os.path.isdir(root_path):
        return rows

    for entry in sorted(os.listdir(root_path)):
        sub = os.path.join(root_path, entry)
        if not os.path.isdir(sub):
            continue

        sensor_id = entry.strip()
        csv_files = sorted([f for f in os.listdir(sub) if f.lower().endswith('.csv')])
        days_count = len(csv_files)  # –ø–æ —É—Å–ª–æ–≤–∏—é = –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Ñ–∞–π–ª–æ–≤
        if days_count == 0:
            continue

        # –≥—Ä–∞–Ω–∏—Ü—ã –ø–æ—è–≤–ª–µ–Ω–∏—è –ø–æ –∫–∞–∂–¥–æ–π —Ç–æ—á–Ω–æ–π –ø–∞—Ä–µ (lat, lon)
        loc_bounds = {}  # (lat, lon) -> [min_ts, max_ts]

        for fname in csv_files:
            fpath = os.path.join(sub, fname)
            try:
                df = _read_sensor_csv(fpath)
            except Exception:
                # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ —Ñ–∞–π–ª—ã, —á—Ç–æ–±—ã –Ω–µ —Ä–æ–Ω—è—Ç—å –ø—Ä–æ—Ü–µ—Å—Å
                continue

            g = df.groupby(['lat', 'lon'])['timestamp'].agg(['min', 'max']).reset_index()
            for _, r in g.iterrows():
                key = (float(r['lat']), float(r['lon']))
                mn, mx = pd.Timestamp(r['min']), pd.Timestamp(r['max'])
                if key not in loc_bounds:
                    loc_bounds[key] = [mn, mx]
                else:
                    if pd.notna(mn) and (pd.isna(loc_bounds[key][0]) or mn < loc_bounds[key][0]):
                        loc_bounds[key][0] = mn
                    if pd.notna(mx) and (pd.isna(loc_bounds[key][1]) or mx > loc_bounds[key][1]):
                        loc_bounds[key][1] = mx

        # —Ñ–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –ø–æ –≤—Å–µ–º –ª–æ–∫–∞—Ü–∏—è–º –¥–∞—Ç—á–∏–∫–∞
        for (lat, lon), (mn, mx) in loc_bounds.items():
            rows.append({
                'sensor_type': sensor_type,                           # –í–í–û–î: —Ç–∏–ø –¥–∞—Ç—á–∏–∫–∞
                'sensor_id': sensor_id,                               # –í–í–û–î: –Ω–æ–º–µ—Ä –¥–∞—Ç—á–∏–∫–∞ (–∏–º—è –ø–æ–¥–ø–∞–ø–∫–∏)
                'days': days_count,                                   # –í–´–ß–ò–°–õ–ï–ù–û: –∫–æ–ª-–≤–æ —Ñ–∞–π–ª–æ–≤ –≤ –ø–∞–ø–∫–µ
                'lat': lat,                                           # –ò–ó CSV: —Ç–æ—á–Ω—ã–π lat
                'lon': lon,                                           # –ò–ó CSV: —Ç–æ—á–Ω—ã–π lon
                'first_seen': mn.strftime('%Y-%m-%dT%H:%M:%S') if pd.notna(mn) else None,
                'last_seen':  mx.strftime('%Y-%m-%dT%H:%M:%S') if pd.notna(mx) else None,
            })

    return rows

# –ó–∞–ø—É—Å–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
all_rows = []
all_rows.extend(process_root(folder_sds, 'SDS011'))
all_rows.extend(process_root(folder_bme, 'BME280'))
df = pd.DataFrame(all_rows)
output_xlsx = 'archive_stats.xlsx'  # –í–´–í–û–î: –∏–º—è –∏—Ç–æ–≥–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞

if df.empty:
    print('‚ö†Ô∏è –ò—Ç–æ–≥–æ–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞ –ø—É—Å—Ç–∞. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—É—Ç–∏ –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ.')
else:
    df = df.sort_values(['sensor_type', 'sensor_id', 'first_seen', 'lat', 'lon']).reset_index(drop=True)
    df.to_excel(output_xlsx, index=False)
    print(f'‚úÖ –ì–æ—Ç–æ–≤–æ: {output_xlsx}  |  —Å—Ç—Ä–æ–∫: {len(df)}')
    
# ____________________–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∞–¥—Ä–µ—Å–∞_______________________

MAPBOX_ENDPOINT = "https://api.mapbox.com/geocoding/v5/mapbox.places/{lon},{lat}.json"
DEFAULT_THREADS = 8
MAX_RETRIES     = 5
TIMEOUT_SEC     = 12

# –§–æ–ª–±—ç–∫–∏ —Ç–∏–ø–æ–≤: –æ—Ç –±–æ–ª–µ–µ —É–∑–∫–∏—Ö –∫ –±–æ–ª–µ–µ —à–∏—Ä–æ–∫–∏–º
FALLBACK_TYPES: List[Optional[str]] = [
    "address,street,neighborhood,locality,place,region,postcode,poi",  # –±–∞–∑–æ–≤—ã–π
    "address,street,place,region,postcode",                             # —É—Å–µ—á—ë–Ω–Ω—ã–π
    None                                                                # –±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–∞ —Ç–∏–ø–æ–≤
]

def _mk_session() -> requests.Session:
    s = requests.Session()
    s.headers.update({"User-Agent": "mapbox-revgeo-ru/1.2"})
    return s

def _sleep_backoff(attempt: int, retry_after: Optional[str] = None) -> None:
    if retry_after:
        try:
            time.sleep(min(60.0, float(retry_after))); return
        except ValueError:
            pass
    time.sleep(0.5 * (2 ** attempt) + random.uniform(0, 0.25))

def _coerce_float(x):
    try:
        if x is None: return None
        if isinstance(x, (float, int)): return float(x)
        return float(str(x).strip().replace(",", "."))
    except Exception:
        return None

def _looks_swapped(lat: Optional[float], lon: Optional[float]) -> bool:
    # –î–ª—è –†–§: —à–∏—Ä–æ—Ç–∞ –æ–±—ã—á–Ω–æ ~40..82, –¥–æ–ª–≥–æ—Ç–∞ ~19..180 –ò–õ–ò ~-180..-169 (–ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ 180-–≥–æ –º–µ—Ä–∏–¥–∏–∞–Ω–∞)
    if lat is None or lon is None: 
        return False
    lat_ok = 40.0 <= lat <= 82.0
    lon_ok = (19.0 <= lon <= 180.0) or (-180.0 <= lon <= -169.0)
    # –ï—Å–ª–∏ lat –ø–æ—Ö–æ–∂ –Ω–∞ –¥–æ–ª–≥–æ—Ç—É –†–§, –∞ lon –ø–æ—Ö–æ–∂ –Ω–∞ —à–∏—Ä–æ—Ç—É –†–§ ‚Äî –≤–µ—Ä–æ—è—Ç–Ω–æ, –ø–æ–º–µ–Ω—è–ª–∏ –º–µ—Å—Ç–∞–º–∏
    lat_like_lon = (19.0 <= lat <= 180.0) or (-180.0 <= lat <= -169.0)
    lon_like_lat = 40.0 <= lon <= 82.0
    return (not lat_ok and not lon_ok) and (lat_like_lon and lon_like_lat)

def _reverse_once(session: requests.Session, token: str, lon: float, lat: float,
                  *, language: str, country: Optional[str], types: Optional[str]) -> Optional[str]:
    url = MAPBOX_ENDPOINT.format(lon=str(lon), lat=str(lat))
    params = {
        "access_token": token,
        "language": language,
        "limit": 1,
    }
    if country:
        params["country"] = country
    if types:
        params["types"] = types

    for attempt in range(MAX_RETRIES):
        try:
            r = session.get(url, params=params, timeout=TIMEOUT_SEC)
            if r.status_code == 200:
                data = r.json()
                feats = data.get("features") or []
                return feats[0].get("place_name") if feats else None
            if r.status_code in (429, 500, 502, 503, 504):
                _sleep_backoff(attempt, r.headers.get("Retry-After")); continue
            if r.status_code in (401, 403):
                raise RuntimeError(f"Mapbox auth error {r.status_code}: {r.text[:200]}")
            if r.status_code in (400, 404, 422):
                return None
            _sleep_backoff(attempt)
        except requests.RequestException:
            _sleep_backoff(attempt)
    return None

def reverse_geocode_point(token: str, lon: float, lat: float,
                          *, language: str = "ru", country: Optional[str] = "ru") -> Optional[str]:
    """–û–¥–∏–Ω –≤—ã–∑–æ–≤ —Å –∫–∞—Å–∫–∞–¥–æ–º —Ñ–æ–ª–±—ç–∫–æ–≤: types ‚Üí –±–µ–∑ types; country ‚Üí –±–µ–∑ country."""
    session = _mk_session()
    # 1) types —Å country
    for t in FALLBACK_TYPES:
        addr = _reverse_once(session, token, lon, lat, language=language, country=country, types=t)
        if addr: 
            return addr
    # 2) —Å–Ω—è—Ç—å country (–∏–Ω–æ–≥–¥–∞ –≥—Ä–∞–Ω–∏—á–Ω—ã–µ —Ç–æ—á–∫–∏)
    for t in FALLBACK_TYPES:
        addr = _reverse_once(session, token, lon, lat, language=language, country=None, types=t)
        if addr:
            return addr
    # 3) –º–∏–∫—Ä–æ-—Å–º–µ—â–µ–Ω–∏–µ (–Ω–∞ —Å–ª—É—á–∞–π –ø–æ–ø–∞–¥–∞–Ω–∏—è –≤ ¬´–Ω–∏—á–µ–π–Ω—ã–π¬ª —Ç–∞–π–ª/–ø–æ–ª–∏–≥–æ–Ω)
    for dx, dy in ((1e-4,0),(-1e-4,0),(0,1e-4),(0,-1e-4)):
        for t in FALLBACK_TYPES:
            addr = _reverse_once(session, token, lon+dx, lat+dy, language=language, country=country, types=t)
            if addr:
                return addr
    return None

def _preflight(token: str) -> None:
    """–ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ–∫–µ–Ω–∞ –Ω–∞ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞—Ö –ö—Ä–µ–º–ª—è; –±—Ä–æ—Å–∞–µ—Ç –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ –ø—Ä–æ–±–ª–µ–º–µ."""
    addr = reverse_geocode_point(token, 37.6175, 55.7520, country="ru")
    if not addr:
        raise RuntimeError("Preflight: –Ω–µ –ø–æ–ª—É—á–∏–ª–∏ –∞–¥—Ä–µ—Å –ø–æ —Ç–µ—Å—Ç–æ–≤–æ–π —Ç–æ—á–∫–µ (–ö—Ä–µ–º–ª—å). "
                           "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—Ä–∞–≤–∞ —Ç–æ–∫–µ–Ω–∞ –Ω–∞ Geocoding –∏ —Å–µ—Ç–µ–≤–æ–π –¥–æ—Å—Ç—É–ø –∫ api.mapbox.com.")

def reverse_geocode_mapbox_bulk(
    df: pd.DataFrame,
    *,
    token: Optional[str] = None,
    lat_col: str = "lat",
    lon_col: str = "lon",
    threads: int = DEFAULT_THREADS,
    language: str = "ru",
    country: Optional[str] = "ru",
    autoswap: bool = True,
    do_preflight: bool = True
) -> pd.Series:

    if token is None:
        token = os.getenv("MAPBOX_TOKEN")
    if not token:
        raise ValueError("–ù–µ—Ç —Ç–æ–∫–µ–Ω–∞. –ü–µ—Ä–µ–¥–∞–π—Ç–µ token=... –∏–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è MAPBOX_TOKEN.")

    if lat_col not in df.columns or lon_col not in df.columns:
        raise ValueError(f"–í df –Ω–µ—Ç –∫–æ–ª–æ–Ω–æ–∫ '{lat_col}' –∏/–∏–ª–∏ '{lon_col}'.")

    lats = df[lat_col].map(_coerce_float)
    lons = df[lon_col].map(_coerce_float)

    if do_preflight:
        _preflight(token)

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∏–º –≤–æ–∑–º–æ–∂–Ω—É—é –ø–µ—Ä–µ—Å—Ç–∞–Ω–æ–≤–∫—É –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–æ—á–∫–∏
    coords = []
    for lat, lon in zip(lats, lons):
        if autoswap and _looks_swapped(lat, lon):
            coords.append((lat, lon, True))   # –æ—Ç–º–µ—á–∞–µ–º –∫–∞–∫ ¬´–ø–µ—Ä–µ–ø—É—Ç–∞–Ω–æ¬ª
        else:
            coords.append((lat, lon, False))

    session = _mk_session()
    cache: Dict[Tuple[float, float], Optional[str]] = {}
    results = [None] * len(df)
    idx_map = list(df.index)

    def _resolve_one(lon: float, lat: float) -> Optional[str]:
        # –ö–∞—Å–∫–∞–¥ —Ñ–æ–ª–±—ç–∫–æ–≤ (–±—ã—Å—Ç—Ä—ã–π –ø—É—Ç—å –±–µ–∑ –ø—Ä–µ—Ñ–ª–∞–π—Ç–∞ –∏ –±–µ–∑ –ª–∏—à–Ω–∏—Ö —Å–µ—Å—Å–∏–π)
        # 1) types + country
        for t in FALLBACK_TYPES:
            a = _reverse_once(session, token, lon, lat, language=language, country=country, types=t)
            if a: return a
        # 2) –±–µ–∑ country
        for t in FALLBACK_TYPES:
            a = _reverse_once(session, token, lon, lat, language=language, country=None, types=t)
            if a: return a
        # 3) –º–∏–∫—Ä–æ-—Å–º–µ—â–µ–Ω–∏—è
        for dx, dy in ((1e-4,0),(-1e-4,0),(0,1e-4),(0,-1e-4)):
            for t in FALLBACK_TYPES:
                a = _reverse_once(session, token, lon+dx, lat+dy, language=language, country=country, types=t)
                if a: return a
        return None

    futures = {}
    with ThreadPoolExecutor(max_workers=max(1, int(threads))) as ex:
        for i, (lat, lon, swapped) in enumerate(coords):
            if lat is None or lon is None or (isinstance(lat, float) and math.isnan(lat)) or (isinstance(lon, float) and math.isnan(lon)):
                results[i] = None; continue

            # –µ—Å–ª–∏ —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ —Å–∫–∞–∑–∞–ª–∞ ¬´–ø–µ—Ä–µ–ø—É—Ç–∞–Ω–æ¬ª ‚Äî –º–µ–Ω—è–µ–º –º–µ—Å—Ç–∞–º–∏
            use_lat, use_lon = (lon, lat) if swapped else (lat, lon)

            # Mapbox –∂–¥—ë—Ç –ø–æ—Ä—è–¥–æ–∫ lon,lat
            key = (use_lon, use_lat)
            if key in cache:
                results[i] = cache[key]; continue
            fut = ex.submit(_resolve_one, use_lon, use_lat)
            futures[fut] = (i, key)

        for fut in as_completed(futures):
            i, key = futures[fut]
            try:
                addr = fut.result()
            except Exception:
                addr = None
            cache[key] = addr
            results[i] = addr

    return pd.Series(results, index=idx_map, name="address_ru")


df["address"] = reverse_geocode_mapbox_bulk(
    df, token=MAPBOX_TOKEN, lat_col="lat", lon_col="lon", threads=8
)
df.to_excel('archive_stats.xlsx', index = False)

# _______________________–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥—Ä—É–≥–∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫_____________________

description = pd.read_excel('description.xlsx')
d1 = description[['–ò–Ω–≤–µ–Ω—Ç–∞—Ä–Ω—ã–π –Ω–æ–º–µ—Ä –∏–∑–¥–µ–ª–∏—è', '–¢–∏–ø', '–ú–∞—Ä–∫–∞',
             '–ù–æ–º–µ—Ä –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞', 'SDS011']]
d1 = d1.rename(columns = {'SDS011': 'sensor_id'}).dropna()

d2 = description[['–ò–Ω–≤–µ–Ω—Ç–∞—Ä–Ω—ã–π –Ω–æ–º–µ—Ä –∏–∑–¥–µ–ª–∏—è', '–¢–∏–ø', '–ú–∞—Ä–∫–∞',
             '–ù–æ–º–µ—Ä –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞', 'BME280']]
d2 = d2.rename(columns = {'BME280': 'sensor_id'}).dropna()
d12 = pd.concat([d1, d2], axis = 0)

def norm_id_to_int(s: pd.Series) -> pd.Series:
    out = s.astype(str).str.extract(r'(\d+)')[0]
    out = pd.to_numeric(out, errors='coerce').astype('Int64')
    return out

df['sensor_id']  = norm_id_to_int(df['sensor_id'])
d12['sensor_id'] = norm_id_to_int(d12['sensor_id'])

all_stats= pd.merge(df, d12, how = 'left', on = 'sensor_id')
all_stats.to_excel('all_stats.xlsx', index = False)



